{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Run setup code\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math\n",
    "import glob\n",
    "import re\n",
    "import collections\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense, Masking,Dropout,Conv1D,TimeDistributed,Flatten,GRU,Bidirectional,MaxPooling1D\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ฆ': 99, 'ฝ': 122, 'f': 70, 'ฤ': 129, '3': 20, '=': 30, 'ั': 141, 'า': 142, '๓': 168, 'ภ': 125, 'ึ': 146, 'ุ': 148, 'x': 89, '&': 7, 'ว': 131, 'y': 90, 'ร': 128, '๒': 167, ']': 62, 'G': 40, 'B': 35, 'ข': 95, 'ฉ': 102, 'ห': 135, '6': 23, 'H': 41, 's': 84, 'โ': 153, 'N': 47, 'F': 39, \"'\": 8, '๔': 169, 'ฑ': 110, '+': 12, '1': 18, '}': 92, 'g': 71, 'ิ': 144, 'o': 79, 'ฟ': 124, 'V': 55, '๕': 170, 'เ': 151, 'j': 74, 'ฯ': 139, 't': 85, '\\\\': 61, '9': 26, 'ญ': 106, '/': 16, 'ก': 94, '็': 158, '๐': 165, '์': 163, '0': 17, 'W': 56, 'e': 69, '>': 31, 'other': 80, 'ใ': 154, 'Y': 58, '^': 63, 'c': 67, 'แ': 152, '‘': 175, 'v': 87, ')': 10, '’': 176, 'บ': 119, 'k': 75, '@': 33, 'u': 86, 'R': 51, ':': 27, 'ผ': 121, ' ': 1, '่': 159, '$': 5, 'จ': 101, 'ฏ': 108, 'd': 68, 'A': 34, '๖': 171, 'ซ': 104, 'K': 44, '\"': 3, 'a': 65, 'ๅ': 156, 'X': 57, 'ธ': 117, 'ฬ': 136, 'ม': 126, '_': 64, 'อ': 137, '!': 2, 'ท': 116, '~': 93, 'ฐ': 109, 'ษ': 133, 'i': 73, 'น': 118, '7': 24, 'ต': 114, 'p': 81, 'z': 91, 'ู': 149, 'ฮ': 138, 'พ': 123, '<': 29, 'ฒ': 111, '๊': 161, 'q': 82, ',': 13, '5': 22, 'b': 66, 'M': 46, '๋': 162, 'D': 37, 'ป': 120, 'r': 83, 'ง': 100, 'Q': 50, 'T': 53, '๘': 173, '2': 19, 'm': 77, '\\ufeff': 177, 'ี': 145, '\\n': 0, 'C': 36, 'I': 42, 'ำ': 143, 'ช': 103, 'ด': 113, ';': 28, '้': 160, 'ฺ': 150, '.': 15, 'S': 52, '?': 32, 'ไ': 155, 'ฌ': 105, 'ื': 147, '#': 4, 'O': 48, 'ะ': 140, 'l': 76, 'ณ': 112, 'ฃ': 96, '*': 11, '8': 25, 'J': 43, '๑': 166, 'w': 88, '-': 14, 'ฎ': 107, '๗': 172, 'n': 78, 'ค': 97, '4': 21, 'h': 72, 'ํ': 164, 'L': 45, 'ถ': 115, '๙': 174, 'ฅ': 98, '%': 6, 'ศ': 132, '[': 60, 'Z': 59, 'ล': 130, 'ๆ': 157, 'P': 49, '(': 9, 'ส': 134, 'U': 54, 'ย': 127, 'E': 38}\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "# Create a character map\n",
    "CHARS = [\n",
    "  '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+',\n",
    "  ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
    "  '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E',\n",
    "  'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n",
    "  'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_',\n",
    "  'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "  'n', 'o', 'other', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
    "  'z', '}', '~', 'ก', 'ข', 'ฃ', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช',\n",
    "  'ซ', 'ฌ', 'ญ', 'ฎ', 'ฏ', 'ฐ', 'ฑ', 'ฒ', 'ณ', 'ด', 'ต', 'ถ', 'ท',\n",
    "  'ธ', 'น', 'บ', 'ป', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ', 'ม', 'ย', 'ร', 'ฤ',\n",
    "  'ล', 'ว', 'ศ', 'ษ', 'ส', 'ห', 'ฬ', 'อ', 'ฮ', 'ฯ', 'ะ', 'ั', 'า',\n",
    "  'ำ', 'ิ', 'ี', 'ึ', 'ื', 'ุ', 'ู', 'ฺ', 'เ', 'แ', 'โ', 'ใ', 'ไ',\n",
    "  'ๅ', 'ๆ', '็', '่', '้', '๊', '๋', '์', 'ํ', '๐', '๑', '๒', '๓',\n",
    "  '๔', '๕', '๖', '๗', '๘', '๙', '‘', '’', '\\ufeff'\n",
    "]\n",
    "CHARS_MAP = {v: k for k, v in enumerate(CHARS)}\n",
    "print(CHARS_MAP)\n",
    "print(len(CHARS_MAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "### my model for tokenize words ###\n",
    "###################################\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Embedding,Dense, Conv1D, Flatten, TimeDistributed, Dropout\n",
    "# from keras.layers import Input, GRU, Bidirectional\n",
    "# from keras.optimizers import Adam\n",
    "# def get_my_best_model2():\n",
    "#     input1 = Input(shape=(21,))\n",
    "#     x = Embedding(178,8)(input1)\n",
    "#     x = Conv1D(100,5,strides=1,activation='relu',padding=\"same\")(x)\n",
    "#     x = TimeDistributed(Dense(5))(x)\n",
    "#     x = Flatten()(x)\n",
    "#     x = Dense(100, activation='relu')(x)\n",
    "#     x = Dropout(0.05)(x)\n",
    "#     x = Dense(100, activation='relu')(x)\n",
    "#     x = Dropout(0.05)(x)\n",
    "#     x = Dense(100, activation='relu')(x)\n",
    "#     x = Dropout(0.05)(x)\n",
    "#     x = Dense(100, activation='relu')(x)\n",
    "#     out = Dense(1, activation='sigmoid')(x)\n",
    "#     model = Model(inputs=input1, outputs=out)\n",
    "#     model.compile(optimizer=Adam(),\n",
    "#                  loss='binary_crossentropy',\n",
    "#                  metrics=['acc'])          \n",
    "#     return model\n",
    "\n",
    "##########################\n",
    "####load model weight ####\n",
    "##########################\n",
    "# weight_path_my_best_model2='/data/model_best2.h5'\n",
    "# my_best_model2 = get_my_best_model2()\n",
    "# my_best_model2.load_weights(weight_path_my_best_model2)\n",
    "# #my_best_model2.make_predict_function()\n",
    "# my_best_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3\n",
      "1    4\n",
      "2    3\n",
      "3    5\n",
      "4    5\n",
      "5    4\n",
      "6    4\n",
      "7    3\n",
      "8    5\n",
      "9    5\n",
      "Name: 1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Path to the preprocessed data\n",
    "best_processed_path = '/data/Wongnai/w_review_train.csv'\n",
    "test_path = '/data/Wongnai/test_file.csv'\n",
    "#Changing .csv file to dataframe without header(0 = review, 1 = rating)\n",
    "df = pd.read_csv(best_processed_path, sep=';', header=None)\n",
    "df_test = pd.read_csv(test_path, sep=';', header=None)\n",
    "# print(\"--------- TRAIN ---------\")\n",
    "# print(df[0][:10],df[1][:10])\n",
    "# print(\"--------- TEST ---------\")\n",
    "# print(df_test[0][:10],df_test[1][:10])\n",
    "print(df[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################\n",
    "## Cleaning data ##\n",
    "###################\n",
    "\n",
    "# ########################## New review data ##########################\n",
    "# new_df = []\n",
    "# for i,x in enumerate (df[0]):\n",
    "#     new_df.append((re.sub(r'[$|.|!|~|=|*|\\n|-|#|+|>|<|)|(|_|^|%|:D|?|,|-|\"|-|฿|0|1|2|3|4|5|6|7|8|9|0|\\t|-|;|:|●|&|@|/|-|{|}|[|]|”|“|]',r'',x)))\n",
    "\n",
    "# ########################## New Test data ##########################\n",
    "# new_test_df = []\n",
    "# for i,x in enumerate (df_test[1]):\n",
    "#     new_test_df.append((re.sub(r'[$|.|!|~|=|*|\\n|-|#|+|>|<|)|(|_|^|%|:D|?|,|-|\"|-|฿|0|1|2|3|4|5|6|7|8|9|0|\\t|-|;|:|●|&|@|/|-|{|}|[|]|”|“|]',r'',x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 3, 5, 5, 4, 4, 3, 5, 5]\n",
      "[[0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Create y_train label data #\n",
    "#############################\n",
    "label = []\n",
    "for rate in df[1]:\n",
    "   label.append(rate)\n",
    "print(label[:10])\n",
    "y_train = pd.get_dummies(pd.Series(label)).as_matrix()\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_n_gram_df(df, n_pad):\n",
    "  \"\"\"\n",
    "  Given an input dataframe, create a feature dataframe of shifted characters\n",
    "  Input:\n",
    "  df: timeseries of size (N)\n",
    "  n_pad: the number of context. For a given character at position [idx],\n",
    "    character at position [idx-n_pad/2 : idx+n_pad/2] will be used \n",
    "    as features for that character.\n",
    "  \n",
    "  Output:\n",
    "  dataframe of size (N * n_pad) which each row contains the character, \n",
    "    n_pad_2 characters to the left, and n_pad_2 characters to the right\n",
    "    of that character.\n",
    "  \"\"\"\n",
    "  n_pad_2 = int((n_pad - 1)/2)\n",
    "  for i in range(n_pad_2):\n",
    "      df['char-{}'.format(i+1)] = df[0].shift(i + 1)\n",
    "      df['char{}'.format(i+1)] = df[0].shift(-i - 1)\n",
    "  return df[n_pad_2: -n_pad_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cutKumKrub(string):\n",
    "    word_test = [x for x in string]\n",
    "    word_test_df2 = pd.DataFrame(word_test)\n",
    "\n",
    "    n_pad = 21\n",
    "    n_pad_2 = int((n_pad - 1)/2)\n",
    "    pad = [{0: ' '}]\n",
    "    df_pad = pd.DataFrame(pad * n_pad_2)\n",
    "\n",
    "    word_test_df2 = pd.concat((df_pad, word_test_df2, df_pad))\n",
    "    word_test_df2[0] = word_test_df2[0].map(lambda x: CHARS_MAP.get(x, 80))\n",
    "    \n",
    "    df_with_context = create_n_gram_df(word_test_df2, n_pad=n_pad)\n",
    "\n",
    "    char_row = ['char' + str(i + 1) for i in range(n_pad_2)] + \\\n",
    "                 ['char-' + str(i + 1) for i in range(n_pad_2)] + [0]\n",
    "\n",
    "    # convert pandas dataframe to numpy array to feed to the model\n",
    "    x_char = df_with_context[char_row].as_matrix()\n",
    "    \n",
    "    y_pred = my_best_model2.predict(x_char)\n",
    "\n",
    "    prob_to_class = lambda p: 1 if p[0]>=0.5 else 0\n",
    "    y_pred = np.apply_along_axis(prob_to_class,1,y_pred)\n",
    "    \n",
    "    word_token = []\n",
    "    word_tmp = ''\n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_pred[i] == 1):\n",
    "            word_token.append(word_tmp)\n",
    "            word_tmp = ''\n",
    "        word_tmp += word_test[i]\n",
    "    else:\n",
    "        word_token.append(word_tmp)\n",
    "    word_token = word_token[1:]\n",
    "    word_token = (\"|\".join(list(filter(lambda x: len(x.replace(\" \", \"\")) > 0, word_token))))\n",
    "    return word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# Cutkum New_cleaing data\n",
    "#########################\n",
    "# new_data2 =[]\n",
    "# for i in tqdm(range(len(new_df))):\n",
    "#     new_data2.append(cutKumKrub(new_df[i]))\n",
    "# print(new_data2[1])\n",
    "# #Train Data\n",
    "#Test Data\n",
    "\n",
    "# test_data2 =[]\n",
    "# for i in tqdm(range(len(new_test_df))):\n",
    "#     test_data2.append(cutKumKrub(new_test_df[i]))\n",
    "# print(test_data2[1])\n",
    "# #Test Data\n",
    "# with open('/data/Wongnai/test_review2.pickle', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(test_data2, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:05<00:00, 6857.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "วัน|นี้|มา|กิน|กัน|ไกล|ถึง|แม่|สอด|จังหวัด|ตากติด|กับ|ชาย|แดน|กัน|ไป|เลย|อำเภอ|นี้|เพราะ|ฉะนั้น|มา|ถึง|ที่|นี่|แล้ว|ก็|ต้อง|กิน|อาหาร|ท้องถิ่น|กัน|หน่อย|อาหาร|ท้องถิ่น|ที่|นี่|มี|หลากหลาย|โดย|เฉพาะ|อาหาร|ที่|มา|จาก|ฝั่ง|เพื่อนบ้าน|ชื่อ|อ่าน|ยาก|มาก|ๆ|ครับ|ดู|ตาม|เมนู|ที่|ผม|โพสต์|ไว้|ได้|เลย|แต่|เมนูอร่อย|ครับ|แปลก|ใหม่|มาก|ๆ|ด้วย|ไม่|เคย|กิน|ที่|ไหน|มา|ก่อน|เป็น|ประสบการณ์|ใหม่|อีก|ร้าน|เลย|ครับ|ร้าน|นี้|มา|แล้ว|ลอง|แวะ|เวียนหา|ประสบการณ์|ใหม่|ใหม่|กัน|นะ|ครับ|อร่อย|ดี|แปลก|ใหม่|ไม่|เหมือน|ใคร|โดย|เฉพาะ|เมนู|แลพัด|โตะ\n",
      "['วัน', 'นี้', 'มา', 'กิน', 'กัน', 'ไกล', 'ถึง', 'แม่', 'สอด', 'จังหวัด', 'ตากติด', 'กับ', 'ชาย', 'แดน', 'กัน', 'ไป', 'เลย', 'อำเภอ', 'นี้', 'เพราะ', 'ฉะนั้น', 'มา', 'ถึง', 'ที่', 'นี่', 'แล้ว', 'ก็', 'ต้อง', 'กิน', 'อาหาร', 'ท้องถิ่น', 'กัน', 'หน่อย', 'อาหาร', 'ท้องถิ่น', 'ที่', 'นี่', 'มี', 'หลากหลาย', 'โดย', 'เฉพาะ', 'อาหาร', 'ที่', 'มา', 'จาก', 'ฝั่ง', 'เพื่อนบ้าน', 'ชื่อ', 'อ่าน', 'ยาก', 'มาก', 'ๆ', 'ครับ', 'ดู', 'ตาม', 'เมนู', 'ที่', 'ผม', 'โพสต์', 'ไว้', 'ได้', 'เลย', 'แต่', 'เมนูอร่อย', 'ครับ', 'แปลก', 'ใหม่', 'มาก', 'ๆ', 'ด้วย', 'ไม่', 'เคย', 'กิน', 'ที่', 'ไหน', 'มา', 'ก่อน', 'เป็น', 'ประสบการณ์', 'ใหม่', 'อีก', 'ร้าน', 'เลย', 'ครับ', 'ร้าน', 'นี้', 'มา', 'แล้ว', 'ลอง', 'แวะ', 'เวียนหา', 'ประสบการณ์', 'ใหม่', 'ใหม่', 'กัน', 'นะ', 'ครับ', 'อร่อย', 'ดี', 'แปลก', 'ใหม่', 'ไม่', 'เหมือน', 'ใคร', 'โดย', 'เฉพาะ', 'เมนู', 'แลพัด', 'โตะ']\n",
      "109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# Loading data_review from disk   #\n",
    "###################################\n",
    "def load_data(path_data):\n",
    "#     data_review = []\n",
    "    with open(path_data, 'rb') as f:\n",
    "        data_review = pickle.load(f)\n",
    "    return data_review\n",
    "\n",
    "data_review = []\n",
    "train_data_path = '/data/Wongnai/data_review2.pickle'\n",
    "data_review = load_data(train_data_path)\n",
    "print(data_review[39999])\n",
    "\n",
    "###########################\n",
    "# Change string to list   #\n",
    "###########################\n",
    "def senToWord(data_review):\n",
    "    temp = []\n",
    "    data_review2 = []\n",
    "    for sentence_index in tqdm(range(len(data_review))):\n",
    "        for y in data_review[sentence_index].split('|'):\n",
    "            #temp.append(y.strip('\\n'))\n",
    "            temp.append(re.sub(r'[$|.|!|~|\\n|-|#|+|>|<]',r'',y))\n",
    "        data_review2.append(temp)\n",
    "        temp = []\n",
    "    return data_review2\n",
    "\n",
    "# data_review2 = []\n",
    "data_review = senToWord(data_review)\n",
    "print(data_review[39999])\n",
    "print(len(data_review[39999]))\n",
    "#print(data_review2[39997][399])\n",
    "#print(data_review2[39997][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6204/6204 [00:00<00:00, 6604.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ร้าน|นี้|จะ|อยู่|เส้น|สัน|กำแพง|-|แม่ออน|เลย|แยก|บ่อสร้าง|ร้าน|จะ|อยู่|ด้าน|ซ้าย|ติด|ริม|ถนน|มี|ป้าย|ติด|ไว้|เห็น|ชัดเจน|ปูทอง|ข้าว|แกง|รส|เด็ด|ตาม|หา|ข้าว|แกง|รสชาติ|นี้|มา|ตลอด|ใน|ที่สุด|ก็|ได้|เจอ|เพราะ|ส่วน|ใหญ่|จะ|เจอรสชาติ|กลาง|ๆ|ถ้า|คน|ชอบรส|จัดจ้าน|แต่|ไม่|ถึง|กับ|เผ็ดเว่อร์|แนะ|นำ|ร้าน|นี้|เลย|ค่ะ|ที่|นั่ง|ก็|สะอาด|สะอ้าน|มี|ทั้ง|น้ำ|ซุป|และ|น้ำพริก|กะปิฟรี|และ|น้ำ|ดื่ม|ฟรีบริการ|ตัว|เอง|คหสต|ชอบ|แบบ|นี้|ที่|ไม่|บังคับ|ให้|ลูกค้า|ต้อง|ซื้อ|น้ำ|กิน|เพราะ|ทาน|คน|เดียว|แค่|ขวด|เหลือ|ทิ้ง|ตลอด|เสียดายเงิน|สั่ง|กับ|ข้าว|ราด|มา|อย่าง|อร่อย|ทั้ง|สอง|อย่าง|เสียดาย|ไข่|ดาว|หมด|ซะ|ก่อน|แต่|แค่|อย่าง|กับ|ข้าว|สวย|ที่|ให้|มา|ใน|ปริมาณ|ที่|อิ่ม|พอดี|ก็|เต็มที่|แล้ว|แม่|ค้า|ก็|อัธยาศัย|ดี|รับรอง|ว่า|มี|กลับ|มา|กิน|อีก|บ่อย|ๆ|แน่นอน|ใคร|ผ่าน|มา|แถว|นี้|ลอง|แวะ|ชิม|นะ|คะ|สำหรับ|คน|ที่|ชอบรสกลม|กล่อม|ไม่|จืด|ชืด\n",
      "['ร้าน', 'นี้', 'จะ', 'อยู่', 'เส้น', 'สัน', 'กำแพง', '-', 'แม่ออน', 'เลย', 'แยก', 'บ่อสร้าง', 'ร้าน', 'จะ', 'อยู่', 'ด้าน', 'ซ้าย', 'ติด', 'ริม', 'ถนน', 'มี', 'ป้าย', 'ติด', 'ไว้', 'เห็น', 'ชัดเจน', 'ปูทอง', 'ข้าว', 'แกง', 'รส', 'เด็ด', 'ตาม', 'หา', 'ข้าว', 'แกง', 'รสชาติ', 'นี้', 'มา', 'ตลอด', 'ใน', 'ที่สุด', 'ก็', 'ได้', 'เจอ', 'เพราะ', 'ส่วน', 'ใหญ่', 'จะ', 'เจอรสชาติ', 'กลาง', 'ๆ', 'ถ้า', 'คน', 'ชอบรส', 'จัดจ้าน', 'แต่', 'ไม่', 'ถึง', 'กับ', 'เผ็ดเว่อร์', 'แนะ', 'นำ', 'ร้าน', 'นี้', 'เลย', 'ค่ะ', 'ที่', 'นั่ง', 'ก็', 'สะอาด', 'สะอ้าน', 'มี', 'ทั้ง', 'น้ำ', 'ซุป', 'และ', 'น้ำพริก', 'กะปิฟรี', 'และ', 'น้ำ', 'ดื่ม', 'ฟรีบริการ', 'ตัว', 'เอง', 'คหสต', 'ชอบ', 'แบบ', 'นี้', 'ที่', 'ไม่', 'บังคับ', 'ให้', 'ลูกค้า', 'ต้อง', 'ซื้อ', 'น้ำ', 'กิน', 'เพราะ', 'ทาน', 'คน', 'เดียว', 'แค่', 'ขวด', 'เหลือ', 'ทิ้ง', 'ตลอด', 'เสียดายเงิน', 'สั่ง', 'กับ', 'ข้าว', 'ราด', 'มา', 'อย่าง', 'อร่อย', 'ทั้ง', 'สอง', 'อย่าง', 'เสียดาย', 'ไข่', 'ดาว', 'หมด', 'ซะ', 'ก่อน', 'แต่', 'แค่', 'อย่าง', 'กับ', 'ข้าว', 'สวย', 'ที่', 'ให้', 'มา', 'ใน', 'ปริมาณ', 'ที่', 'อิ่ม', 'พอดี', 'ก็', 'เต็มที่', 'แล้ว', 'แม่', 'ค้า', 'ก็', 'อัธยาศัย', 'ดี', 'รับรอง', 'ว่า', 'มี', 'กลับ', 'มา', 'กิน', 'อีก', 'บ่อย', 'ๆ', 'แน่นอน', 'ใคร', 'ผ่าน', 'มา', 'แถว', 'นี้', 'ลอง', 'แวะ', 'ชิม', 'นะ', 'คะ', 'สำหรับ', 'คน', 'ที่', 'ชอบรสกลม', 'กล่อม', 'ไม่', 'จืด', 'ชืด']\n",
      "173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "## load test Data for get score\n",
    "###############################\n",
    "#Load test path\n",
    "test_review = []\n",
    "test_data_path = '/data/Wongnai/test_review2.pickle'\n",
    "test_review = load_data(test_data_path)\n",
    "print(test_review[1])\n",
    "\n",
    "# test_review2 = []\n",
    "test_review = senToWord(test_review)\n",
    "test_review = test_review[1:]\n",
    "print(test_review[0])\n",
    "print(len(test_review[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# Make embedding # Now use thai2vec\n",
    "##################\n",
    "# from tqdm import tqdm\n",
    "# def file_len(fname):\n",
    "#     with open(fname) as f:\n",
    "#         for i, l in enumerate(f):\n",
    "#             pass\n",
    "#     return i + 1\n",
    "\n",
    "# word2vec = dict()\n",
    "# fileLen = file_len('/data/thai2vec.vec')\n",
    "# with open('/data/thai2vec.vec', 'r') as f:\n",
    "#     #f.seek(0)\n",
    "#     for index, line in tqdm(enumerate(f), total = fileLen):\n",
    "#         if index == 0:\n",
    "#             continue\n",
    "#         word2vec[line.split(' ')[0]] = [float(x) for x in line.split(' ')[1:]]\n",
    "# print(len(word2vec))\n",
    "# with open('/data/Wongnai/embedding2.pickle', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(word2vec, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Total unique word] = 129802\n",
      "129801\n",
      "[UNK word] = 82.19056717153819\n",
      "[('เบค่อนกรอบ', 4), ('บางขุนศรี', 4), ('Tsui', 4), ('Nama', 4), ('นไป', 4), ('เค้กตัด', 4), ('update', 4), ('frappucino', 4), ('สเต็คปลา', 4), ('เฉียดร้อย', 4)]\n",
      "[Unique word] = 23118\n"
     ]
    }
   ],
   "source": [
    "words =[]\n",
    "for sentence in data_review:\n",
    "    for word in sentence:\n",
    "        words.append(word)\n",
    "word_count = list()\n",
    "word_count.extend(collections.Counter(words).most_common(len(set(words))))\n",
    "print(\"[Total unique word] =\",len(word_count))\n",
    "threshold = 3\n",
    "\n",
    "num_UNK = 0\n",
    "index = len(word_count) - 1\n",
    "print(index)\n",
    "rare_word = set()\n",
    "while(word_count[index][1] <= threshold):\n",
    "    num_UNK += word_count[index][1]\n",
    "    rare_word.add(word_count[index][0])\n",
    "    index -= 1\n",
    "print(\"[UNK word] =\",len(rare_word)*100/len(word_count)) \n",
    "\n",
    "word_count = word_count[:index+1]\n",
    "word_count.append((\"UNK\",num_UNK))\n",
    "word_count = sorted(word_count, key=lambda x: -x[1])\n",
    "\n",
    "#print out 10 most frequent words\n",
    "print(word_count[-10:])\n",
    "dictionary = dict()\n",
    "dictionary[\"for_keras_zero_padding\"] = 0\n",
    "for word in word_count:\n",
    "    dictionary[word[0]] = len(dictionary)\n",
    "data = list()\n",
    "data_feature = []\n",
    "for x in data_review:\n",
    "    for word in x:\n",
    "        if(word not in rare_word):\n",
    "            data.append(dictionary[word])\n",
    "        else:\n",
    "            data.append(dictionary[\"UNK\"])\n",
    "    data_feature.append(data)\n",
    "    data =[]\n",
    "# print(list(map(lambda lis: list(map(lambda y: index_to_word[y], lis)), data_feature[:2])))\n",
    "print(\"[Unique word] =\",len(word_count))\n",
    "del words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51358\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# Loading embedding Once time #\n",
    "###############################\n",
    "embedding_path = '/data/Wongnai/embedding2.pickle'\n",
    "pre_em = load_data(embedding_path)\n",
    "print(len(pre_em))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word to index, index to key form fastText emb\n",
    "word_to_index = dict()\n",
    "index_to_word = dict()\n",
    "for idx, x in enumerate(pre_em.keys()):\n",
    "    word_to_index[x] = idx\n",
    "    index_to_word[idx] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23119\n",
      "51358\n",
      "match word = 7479 percent = 32.35001513906311\n"
     ]
    }
   ],
   "source": [
    "#Check different words between DataSet and Embed\n",
    "print(len(dictionary)) # Dataset\n",
    "print(len(word_to_index)) # \n",
    "count = 0\n",
    "notmatch_word =[]\n",
    "for i in dictionary.keys():\n",
    "    if(i in word_to_index):\n",
    "        count+=1\n",
    "    else:\n",
    "        notmatch_word.append(i)\n",
    "print(\"match word =\",count,\"percent =\",count*100/len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23120, 300)\n"
     ]
    }
   ],
   "source": [
    "# Create embedding Main on review train_data\n",
    "pre_embedding = []\n",
    "index =[]\n",
    "pre_embedding.append(np.zeros(300))\n",
    "for i,l in enumerate (dictionary.keys()):\n",
    "    if(l in word_to_index):\n",
    "        index.append(l)\n",
    "        pre_embedding.append(pre_em[l])\n",
    "    else:\n",
    "        pre_embedding.append(np.zeros(300))\n",
    "weight_em = np.array(pre_embedding)\n",
    "print(weight_em.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Create embedding Main train Embeding ### This mean Original Embedding + unknow_words Union\n",
    "# pre_embedding_ftext = []\n",
    "# pre_embedding_ftext.append(np.zeros(300))\n",
    "# for i,sen in enumerate (pre_em):\n",
    "#     pre_embedding_ftext.append(pre_em[sen])\n",
    "# # print(pre_embedding_ftext[1])\n",
    "# #-------------------------------------------\n",
    "# print(len(dictionary)) # Dataset\n",
    "# print(len(word_to_index)) # \n",
    "# count = 0\n",
    "# notmatch_word =[]\n",
    "# for i in dictionary.keys():\n",
    "#     if(i in word_to_index.keys()):\n",
    "#         count+=1\n",
    "#     else:\n",
    "#         notmatch_word.append(i)\n",
    "# print(\"match word =\",count,\"percent =\",count*100 /len(dictionary))\n",
    "# #--------------------------------------------\n",
    "# word_to_index2 = []\n",
    "# word_to_index2 = word_to_index\n",
    "# for i,l in enumerate (dictionary.keys()):\n",
    "#     if(not l in word_to_index2):\n",
    "#         word_to_index2[l] = len(word_to_index2)\n",
    "#         pre_embedding_ftext.append(np.zeros(300)) \n",
    "# #---------------------------------------------\n",
    "# print(len(dictionary)) # Dataset\n",
    "# print(len(word_to_index2)) # \n",
    "# count = 0\n",
    "# notmatch_word =[]\n",
    "# for i in dictionary.keys():\n",
    "#     if(i in word_to_index2.keys()):\n",
    "#         count+=1\n",
    "#     else:\n",
    "#         notmatch_word.append(i)\n",
    "# print(\"match word =\",count,\"percent =\",count*100 /len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word_to_index2['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #prepare feature now use thai2vec\n",
    "# data_feature_ftext= []\n",
    "# data =[]\n",
    "# for sentence in data_review:\n",
    "#     for word in sentence:\n",
    "#         if(word in word_to_index2):\n",
    "#             data.append(word_to_index2[word])\n",
    "#         else:\n",
    "#             data.append(word_to_index2[\"UNK\"])\n",
    "#     data_feature_ftext.append(data)\n",
    "#     data =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(len(pre_embedding_ftext),pre_embedding_ftext[130469])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weight_em_ftext = np.array(pre_embedding_ftext)\n",
    "# con = []\n",
    "# count=0\n",
    "# weight_em_ftext_new  =[]\n",
    "# for i,word in enumerate (weight_em_ftext):\n",
    "#     if (len(word) != 300):\n",
    "#         weight_em_ftext_new.append(np.zeros(300))\n",
    "#     else:\n",
    "#         weight_em_ftext_new.append(weight_em_ftext[i])\n",
    "\n",
    "# weight_em_ftext_new = np.array(weight_em_ftext_new)\n",
    "# print(count, con, weight_em_ftext_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 1000)\n"
     ]
    }
   ],
   "source": [
    "#Make padding for train data OLDDDD  ###################################################################################\n",
    "def pad(A, npads):\n",
    "    _npads = npads - len(A)\n",
    "    return np.pad(A, pad_width=_npads, mode='constant', constant_values=0)[_npads:]\n",
    "\n",
    "x_train = sequence.pad_sequences(data_feature,maxlen =1000, padding= 'post', truncating='pre')\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6203, 1000)\n"
     ]
    }
   ],
   "source": [
    "#Make input feature for test data OLDDDDDDDDD ###########################################################################\n",
    "data =[]\n",
    "test_feature = []\n",
    "for x in test_review:\n",
    "    for word in x:\n",
    "        if(word in dictionary.keys()):\n",
    "            data.append(dictionary[word])\n",
    "        else:\n",
    "            data.append(dictionary[\"UNK\"])\n",
    "    test_feature.append(data)\n",
    "    data =[]\n",
    "test =[]\n",
    "\n",
    "test = sequence.pad_sequences(test_feature,maxlen =1000, padding= 'post', truncating='pre')\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Make padding for train data NEWWWWWWWWWWW\n",
    "# x_train = np.array(data_feature_ftext)\n",
    "# x_train2 = []\n",
    "# for i in range (len(x_train)):\n",
    "#     x_train2.append(pad(x_train[i],400))\n",
    "# x_train = np.array(x_train2)\n",
    "# print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Make input feature for test data NEW\n",
    "# data_test_ftext= []\n",
    "# data =[]\n",
    "# for sentence in test_review:\n",
    "#     for word in sentence:\n",
    "#         if(word in word_to_index2):\n",
    "#             data.append(word_to_index2[word])\n",
    "#         else:\n",
    "#             data.append(word_to_index2[\"UNK\"])\n",
    "#     data_test_ftext.append(data)\n",
    "#     data =[]\n",
    "# test =[]\n",
    "# for i in range (len(data_test_ftext)):\n",
    "#     test.append(pad(x_train[i],400))\n",
    "# test = np.array(test)\n",
    "# print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Prepare data for training\n",
    "input_data = x_train\n",
    "train_data = input_data[:int(len(input_data)*0.7)]\n",
    "val_data = input_data[int(len(input_data)*0.7):int(len(input_data)*0.9)]\n",
    "test_data_ev = input_data[int(len(input_data)*0.9):]\n",
    "\n",
    "target = y_train\n",
    "train_target = y_train[:int(len(target)*0.7)]\n",
    "val_target = y_train[int(len(target)*0.7):int(len(target)*0.9)]\n",
    "test_target_ev = label[int(len(target)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "def evaluate(x_test, y_test, model):\n",
    "    \"\"\"\n",
    "    Evaluate model on the splitted 10 percent testing set.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(x_test)\n",
    "    correct = 0\n",
    "    #map probability to class\n",
    "    y_pred_mapped = []\n",
    "    for i,pred in enumerate(y_pred):\n",
    "        pred = list(pred)\n",
    "        y_pred_mapped.append(pred.index(max(pred))+1)\n",
    "\n",
    "#             y_pred = np.apply_along_axis(prob_to_class,1,y_pred)\n",
    "\n",
    "    f1score = f1_score(y_test,y_pred_mapped, average='weighted')\n",
    "    precision = precision_score(y_test,y_pred_mapped, average='weighted')\n",
    "    recall = recall_score(y_test,y_pred_mapped, average='weighted')\n",
    "    return f1score, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_my_best_model_br():\n",
    "#     input1 = Input(shape=(1000,))\n",
    "#     x = Embedding(len(dictionary)+1,300, weights = [weight_em])(input1)\n",
    "#     x = Conv1D(64,5,strides=1,activation='relu',padding=\"valid\")(x)\n",
    "#     x = MaxPooling1D(pool_size=5, strides=1, padding='valid')(x)\n",
    "#     x = Conv1D(32,5,strides=1,activation='relu',padding=\"valid\")(x)\n",
    "#     x = MaxPooling1D(pool_size=5, strides=1, padding='valid')(x)\n",
    "# #     x = TimeDistributed(Dense(10))(x)\n",
    "# #     x = Bidirectional(GRU(50))(x)\n",
    "#     x = Dropout(0.25)(x)\n",
    "#     x = TimeDistributed(Dense(5))(x)\n",
    "#     x = Flatten()(x)\n",
    "#     x = Dense(100, activation='relu')(x)\n",
    "#     x = Dense(100, activation='relu')(x)\n",
    "# #     x = Dropout(0.4)(x)\n",
    "#     out = Dense(5, activation='softmax')(x)\n",
    "#     model = Model(inputs=input1, outputs=out)\n",
    "#     model.compile(optimizer=Adam(),\n",
    "#                  loss='categorical_crossentropy',\n",
    "#                  metrics=['categorical_accuracy'])\n",
    "#     return model\n",
    "# # #load model weight\n",
    "# # weight_path_my_best_model2='/data/model_best2.h5'\n",
    "# model = get_my_best_model_br()\n",
    "# # my_best_model2.load_weights(weight_path_my_best_model2)\n",
    "# #model.make_predict_function()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %time\n",
    "# weight_path_model='/data/Wongnai/my_weight_CNN.h5'\n",
    "# callbacks_list_model = [\n",
    "#     ModelCheckpoint(\n",
    "#         weight_path_model,\n",
    "#         monitor = \"val_loss\",\n",
    "#         mode = 'min',\n",
    "#         verbose = 1,\n",
    "#         save_best_only = True,\n",
    "#         save_weights_only = True,\n",
    "#     )   \n",
    "# ]\n",
    "# model.fit(train_data,train_target,batch_size=256,epochs=5,verbose=1,callbacks=callbacks_list_model,\n",
    "#           validation_data=(val_data, val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # #load model weight\n",
    "# weight_path_my_best_model='/data/Wongnai/my_weight_CNN.h5'\n",
    "# model = get_my_best_model_br()\n",
    "# model.load_weights(weight_path_my_best_model)\n",
    "# # #my_best_model2.make_predict_function()\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # test_target_ev = np.argmax(test_target_ev, axis=1)\n",
    "# evaluate(test_data_ev,test_target_ev,model)\n",
    "# y_pred=model.predict(test) \n",
    "# print(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ans = np.argmax(y_pred, axis=1)\n",
    "# ans = [x+1 for x in ans]\n",
    "# d =dict()\n",
    "# d['reviewID'] = [x for x in range (1,6204)]\n",
    "# d['rating'] = ans\n",
    "# sub= pd.DataFrame(d)\n",
    "# # sub.to_csv('/data/BestCnn.csv', ',', index=False , columns=['reviewID','rating'])\n",
    "# sub.to_csv('/data/BestCnn.csv', ',', index=False , columns=['reviewID','rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 1000, 300)         6936000   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 996, 32)           48032     \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 992, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 988, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 988, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 31616)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 75)                2371275   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 75)                5700      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 380       \n",
      "=================================================================\n",
      "Total params: 9,366,539\n",
      "Trainable params: 9,366,539\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    input1 = Input(shape=(1000,))\n",
    "    x = Embedding(len(dictionary)+1,300, weights = [weight_em])(input1)\n",
    "    x = Conv1D(32,5,strides=1,activation='relu',padding=\"valid\")(x)\n",
    "    x = Conv1D(32,5,strides=1,activation='relu',padding=\"valid\")(x)\n",
    "    x = MaxPooling1D(pool_size=5, strides=1, padding='valid')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(75, activation='relu')(x)\n",
    "    x = Dense(75, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    out = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.68 µs\n",
      "Train on 28000 samples, validate on 8000 samples\n",
      "Epoch 1/5\n",
      "27904/28000 [============================>.] - ETA: 0s - loss: 1.2355 - categorical_accuracy: 0.4560Epoch 00000: val_loss improved from inf to 1.16109, saving model to /data/Wongnai/my_weight_test.h5\n",
      "28000/28000 [==============================] - 19s - loss: 1.2354 - categorical_accuracy: 0.4560 - val_loss: 1.1611 - val_categorical_accuracy: 0.4796\n",
      "Epoch 2/5\n",
      "27904/28000 [============================>.] - ETA: 0s - loss: 1.0726 - categorical_accuracy: 0.5202Epoch 00001: val_loss improved from 1.16109 to 0.99789, saving model to /data/Wongnai/my_weight_test.h5\n",
      "28000/28000 [==============================] - 18s - loss: 1.0723 - categorical_accuracy: 0.5203 - val_loss: 0.9979 - val_categorical_accuracy: 0.5455\n",
      "Epoch 3/5\n",
      "27904/28000 [============================>.] - ETA: 0s - loss: 0.8863 - categorical_accuracy: 0.6059Epoch 00002: val_loss improved from 0.99789 to 0.99541, saving model to /data/Wongnai/my_weight_test.h5\n",
      "28000/28000 [==============================] - 18s - loss: 0.8864 - categorical_accuracy: 0.6063 - val_loss: 0.9954 - val_categorical_accuracy: 0.5496\n",
      "Epoch 4/5\n",
      "27904/28000 [============================>.] - ETA: 0s - loss: 0.6530 - categorical_accuracy: 0.7282Epoch 00003: val_loss did not improve\n",
      "28000/28000 [==============================] - 18s - loss: 0.6533 - categorical_accuracy: 0.7279 - val_loss: 1.1619 - val_categorical_accuracy: 0.5179\n",
      "Epoch 5/5\n",
      "27904/28000 [============================>.] - ETA: 0s - loss: 0.4076 - categorical_accuracy: 0.8446Epoch 00004: val_loss did not improve\n",
      "28000/28000 [==============================] - 18s - loss: 0.4076 - categorical_accuracy: 0.8446 - val_loss: 1.4288 - val_categorical_accuracy: 0.5145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3e0bbb2cc0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "weight_path_model='/data/Wongnai/my_weight_test.h5'\n",
    "callbacks_list_model = [\n",
    "   # TensorBoard(log_dir='/data/Graph/gru', histogram_freq=1, write_graph=True, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "        weight_path_model,\n",
    "        monitor = \"val_loss\",\n",
    "        mode = 'min',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "    )   \n",
    "]\n",
    "\n",
    "# model.fit(x_train_char,y_train,batch_size=4096, epochs=3, verbose=2,\n",
    "#              callbacks=callbacks_list_model, validation_data=(x_val_char,y_val))\n",
    "model.fit(train_data,train_target,batch_size=128,epochs=5,verbose=1,callbacks=callbacks_list_model,\n",
    "          validation_data=(val_data, val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #load model weight\n",
    "weight_path_my_best_model='/data/Wongnai/my_weight_test.h5'\n",
    "model = get_model()\n",
    "model.load_weights(weight_path_my_best_model)\n",
    "# #my_best_model2.make_predict_function()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekapolc/.env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ekapolc/.env/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.48847782918082611, 0.52004143782873735, 0.53600000000000003)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate(test_data_ev,test_target_ev,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.42610281e-06   6.64945284e-04   1.18724301e-01   5.64647496e-01\n",
      "   3.15958798e-01]\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(test) \n",
    "print(y_pred[0])\n",
    "ans = np.argmax(y_pred, axis=1)\n",
    "ans = [x+1 for x in ans]\n",
    "d = dict()\n",
    "d['reviewID'] = [x for x in range (1,6204)]\n",
    "d['rating'] = ans\n",
    "sub= pd.DataFrame(d)\n",
    "sub.to_csv('/data/Cnn1.csv', ',', index=False , columns=['reviewID','rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
